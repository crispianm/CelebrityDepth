{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import torch\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import save_checkpoint, load_checkpoint, save_some_examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "disc = Discriminator(in_channels=3).to(config.DEVICE)\n",
    "gen = Generator(in_channels=3, features=64).to(config.DEVICE)\n",
    "\n",
    "opt_disc = optim.Adam(\n",
    "    disc.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "opt_gen = optim.Adam(\n",
    "    gen.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "BCE = nn.BCEWithLogitsLoss()\n",
    "L1_LOSS = nn.L1Loss()\n",
    "\n",
    "load_checkpoint(\n",
    "    \"./checkpoints/gen_0.pth.tar\",\n",
    "    gen,\n",
    "    opt_gen,\n",
    "    config.LEARNING_RATE,\n",
    ")\n",
    "load_checkpoint(\n",
    "    \"./checkpoints/disc_0.pth.tar\",\n",
    "    disc,\n",
    "    opt_disc,\n",
    "    config.LEARNING_RATE,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depth_path = './celeba/depth_val/000001_depth.png'\n",
    "face_path = './celeba/align_val/000001.jpg'\n",
    "depth = Image.open(depth_path).convert('RGB')\n",
    "face = Image.open(face_path).convert('RGB')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),  # Resize the images to a smaller size\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomHorizontalFlip(),  # Flip the images horizontally\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "x = transform(depth)\n",
    "x =  torch.unsqueeze(x, 0)\n",
    "\n",
    "y = transform(face)\n",
    "y =  torch.unsqueeze(y, 0)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def save_some_examples(gen, x, y, epoch=0, folder='./evaluation/'):\n",
    "\n",
    "    x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
    "        if epoch == 1:\n",
    "            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
    "    gen.train()\n",
    "\n",
    "save_some_examples(gen, x, y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5922, -0.5922, -0.5922,  ...,  0.2784,  0.2314,  0.1686],\n",
       "         [-0.5922, -0.5922, -0.5922,  ...,  0.2863,  0.2706,  0.2627],\n",
       "         [-0.6000, -0.5922, -0.5922,  ...,  0.2863,  0.2863,  0.2863],\n",
       "         ...,\n",
       "         [ 0.9608,  0.9608,  0.9608,  ...,  0.4588,  0.4588,  0.4667],\n",
       "         [ 0.9529,  0.9608,  0.9608,  ...,  0.4667,  0.4667,  0.4745],\n",
       "         [ 0.9451,  0.9529,  0.9608,  ...,  0.4745,  0.4824,  0.4902]],\n",
       "\n",
       "        [[-0.9686, -0.9686, -0.9686,  ..., -0.7725, -0.8118, -0.8667],\n",
       "         [-0.9686, -0.9686, -0.9686,  ..., -0.7647, -0.7725, -0.7882],\n",
       "         [-0.9686, -0.9686, -0.9686,  ..., -0.7647, -0.7647, -0.7647],\n",
       "         ...,\n",
       "         [ 0.2157,  0.2314,  0.2314,  ..., -0.5922, -0.5922, -0.5843],\n",
       "         [ 0.2078,  0.2235,  0.2314,  ..., -0.5843, -0.5843, -0.5765],\n",
       "         [ 0.1922,  0.2078,  0.2235,  ..., -0.5765, -0.5686, -0.5608]],\n",
       "\n",
       "        [[ 0.1922,  0.1922,  0.1922,  ...,  0.2000,  0.2314,  0.2627],\n",
       "         [ 0.1922,  0.1922,  0.1922,  ...,  0.1922,  0.2000,  0.2078],\n",
       "         [ 0.1843,  0.1922,  0.1922,  ...,  0.1922,  0.1922,  0.1922],\n",
       "         ...,\n",
       "         [-0.5373, -0.5451, -0.5451,  ...,  0.0588,  0.0588,  0.0510],\n",
       "         [-0.5294, -0.5373, -0.5451,  ...,  0.0510,  0.0510,  0.0431],\n",
       "         [-0.5137, -0.5294, -0.5373,  ...,  0.0431,  0.0353,  0.0275]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),  # Resize the images to a smaller size\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomHorizontalFlip(),  # Flip the images horizontally\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth = depth.expand(3,*depth.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gen.pth.tar_'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "\n",
    "str(config.CHECKPOINT_GEN + \"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
