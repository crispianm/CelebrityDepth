{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import torch\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import save_checkpoint, load_checkpoint, save_some_examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "disc = Discriminator(in_channels=3).to(config.DEVICE)\n",
    "gen = Generator(in_channels=3, features=64).to(config.DEVICE)\n",
    "\n",
    "opt_disc = optim.Adam(\n",
    "    disc.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "opt_gen = optim.Adam(\n",
    "    gen.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "BCE = nn.BCEWithLogitsLoss()\n",
    "L1_LOSS = nn.L1Loss()\n",
    "\n",
    "load_checkpoint(\n",
    "    \"./checkpoints/gen_0.pth.tar\",\n",
    "    gen,\n",
    "    opt_gen,\n",
    "    config.LEARNING_RATE,\n",
    ")\n",
    "load_checkpoint(\n",
    "    \"./checkpoints/disc_0.pth.tar\",\n",
    "    disc,\n",
    "    opt_disc,\n",
    "    config.LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depth_path = './celeba/d_test/000001_depth.png'\n",
    "face_path = './celeba/a_test/000001.jpg'\n",
    "depth = Image.open(depth_path).convert('RGB')\n",
    "face = Image.open(face_path).convert('RGB')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),  # Resize the images to a smaller size\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomHorizontalFlip(),  # Flip the images horizontally\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "depth = transform(depth)\n",
    "depth = torch.unsqueeze(depth, 0)\n",
    "\n",
    "face = transform(face)\n",
    "face =  torch.unsqueeze(face, 0)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def save_some_examples(gen, x, y, epoch=0, folder='./evaluation/manual_saves'):\n",
    "\n",
    "    x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
    "        if epoch == 1:\n",
    "            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
    "    gen.train()\n",
    "\n",
    "save_some_examples(gen, depth, face, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
